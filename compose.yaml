
services:
  # 1. Votre application Python (Streamlit + PySpark Driver)
  app:
    build: .  # Construit à partir du Dockerfile dans le même dossier
    ports:
      - "8501:8501"  # Port pour Streamlit
    volumes:
      - .:/app  # Synchronise votre code local avec le conteneur
    depends_on:
      - spark-master
    environment:
      - SPARK_MASTER_URL=spark://spark-master:7077
      - SPARK_HOME=/opt/spark
      - PYTHONPATH=$SPARK_HOME/python:$SPARK_HOME/python/lib/py4j-0.10.9.7-src.zip:$PYTHONPATH
    command: streamlit run main.py --server.fileWatcherType=poll

  # 2. Le "Master" Apache Spark
  spark-master:
    image: docker.io/apache/spark:3.5.0
    ports:
      - "8088:8080"  # UI Web de Spark
      - "7077:7077"  # Port du cluster
    command: /opt/spark/bin/spark-class org.apache.spark.deploy.master.Master
    volumes:
      - .:/app


  # 3. Un "Worker" Apache Spark
  spark-worker:
    image: docker.io/apache/spark:3.5.0
    depends_on:
      - spark-master
    command: /opt/spark/bin/spark-class org.apache.spark.deploy.worker.Worker spark://spark-master:7077
    volumes: # <-- for finding the files that I've added
      - .:/app
    environment:
      - SPARK_WORKER_MEMORY=2g
  airflow:
    build: ./airflow
    container_name: logistic_airflow
    restart: always
    environment:
      AIRFLOW__CORE__EXECUTOR: SequentialExecutor
      AIRFLOW__CORE__LOAD_EXAMPLES: "False"
      AIRFLOW__WWW_RBAC__USERNAME: admin
      AIRFLOW__WWW_RBAC__PASSWORD: admin
    volumes:
      - ./airflow/dags:/opt/airflow/dags
    ports:
      - "8081:8080"
    command: [ "standalone" ]


